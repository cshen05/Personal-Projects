geom_boxplot(aes(x=group, y=severity_diff))
acupuncture <- data %>%
group_by(group) %>%
filter(group == 'Acupuncture')
median(acupuncture$severity_diff, na.rm=TRUE)
IQR(acupuncture$severity_diff, na.rm=TRUE)
range(acupuncture$severity_diff, na.rm=TRUE)
control <- data %>%
group_by(group) %>%
filter(group == 'Control')
median(control$severity_diff, na.rm=TRUE)
IQR(control$severity_diff, na.rm=TRUE)
range(control$severity_diff, na.rm=TRUE)
my_glm <- glm(Heart_disease~sysBP, data = heartdisease_train, family = "binomial")
setwd("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Classification")
my_glm <- glm(Heart_disease~sysBP, data = heartdisease_train, family = "binomial")
source("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Classification/Logistic_Regression_Heart.R", echo=TRUE)
my_glm <- glm(Heart_disease~totChol, data = heartdisease_train, family = "binomial")
summary(my_glm)
predicted_glm_train <- predict(my_glm, heartdisease_train, type = "response")
yhat_predict_train <- ifelse(predicted_glm_train > 0.5, 1, 0) # We compare the predicted values of heart disease to 0.5 and output 0 if they less then 0.5 and 1 if they are larger than 0.5
## Create a table
table_heart_train <- table(y = heartdisease_train$Heart_disease, yhat = yhat_predict_train)
table_heart_train
## Classification accuracy
accuracy_heart_train <- sum(diag(table_heart_train))/ sum(table_heart_train)
accuracy_heart_train
my_glm <- glm(Heart_disease~totChol, data = heartdisease_train, family = "binomial")
summary(my_glm)
# Visualization within the range of data
logis_plot <- ggplot(heartdisease_train, aes(x=totChol, y=Heart_disease)) + geom_point(size=2, color = "blue", shape=19)
logis_plot + geom_smooth(method='glm', se = FALSE, colour="red", fullrange=TRUE, method.args = list(family= "binomial"))
# Visualization outside the range of data
logis_plot <- ggplot(heartdisease_train, aes(x=totChol, y=Heart_disease)) + geom_point(size=2, color = "blue", shape=19)
logis_plot + geom_smooth(method='glm', se = FALSE, colour="red", fullrange=TRUE, method.args = list(family= "binomial")) + xlim(0, 350)
################### Compute the classification accuracy from training data
predicted_glm_train <- predict(my_glm, heartdisease_train, type = "response")
yhat_predict_train <- ifelse(predicted_glm_train > 0.5, 1, 0) # We compare the predicted values of heart disease to 0.5 and output 0 if they less then 0.5 and 1 if they are larger than 0.5
## Create a table
table_heart_train <- table(y = heartdisease_train$Heart_disease, yhat = yhat_predict_train)
table_heart_train
## Classification accuracy
accuracy_heart_train <- sum(diag(table_heart_train))/ sum(table_heart_train)
accuracy_heart_train
#################### Compute the classification accuracy from test data
predicted_glm_test <- predict(my_glm, heartdisease_test, type = "response")
yhat_predict_test <- ifelse(predicted_glm_test > 0.5, 1, 0)
table_heart_test <- table(y = heartdisease_test$Heart_disease, yhat = yhat_predict_test)
table_heart_test
## Classification accuracy from test data
accuracy_heart_test <- sum(diag(table_heart_test))/ sum(table_heart_test)
accuracy_heart_test
my_glm <- glm(Heart_disease~heartRate, data = heartdisease_train, family = "binomial")
summary(my_glm)
predicted_glm_train <- predict(my_glm, heartdisease_train, type = "response")
yhat_predict_train <- ifelse(predicted_glm_train > 0.5, 1, 0) # We compare the predicted values of heart disease to 0.5 and output 0 if they less then 0.5 and 1 if they are larger than 0.5
## Create a table
table_heart_train <- table(y = heartdisease_train$Heart_disease, yhat = yhat_predict_train)
table_heart_train
## Classification accuracy
accuracy_heart_train <- sum(diag(table_heart_train))/ sum(table_heart_train)
accuracy_heart_train
predicted_glm_test <- predict(my_glm, heartdisease_test, type = "response")
yhat_predict_test <- ifelse(predicted_glm_test > 0.5, 1, 0)
table_heart_test <- table(y = heartdisease_test$Heart_disease, yhat = yhat_predict_test)
table_heart_test
## Classification accuracy from test data
accuracy_heart_test <- sum(diag(table_heart_test))/ sum(table_heart_test)
accuracy_heart_test
source("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Classification/Spam_Email.R", echo=TRUE)
library(readr)
spam_train <- read_csv("spam_train.csv")
View(spam_train)
library(readr)
spam_test <- read_csv("spam_test.csv")
View(spam_test)
source("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Classification/Spam_Email.R", echo=TRUE)
library(readr)
matches <- read_csv("~/Documents/GitHub/Personal Projects/R/SDS320/matches.csv")
View(matches)
data <- read.csv("matches.csv")
setwd("~/Documents/GitHub/Personal Projects/R/SDS320")
data <- read.csv("matches.csv")
max(data$xg)
setwd("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Classification")
library(ggplot2)
library(tidyr)
library(dplyr)
library(FNN)
################################################################################
############## We first try with the "Heart_disease.csv"
heartdisease = read.csv("Heart_disease.csv", head = TRUE, check.names=FALSE)
heartdisease <- data.frame(heartdisease)
colnames(heartdisease)[16] <- c("Heart_disease") # Change the column name of Y
head(heartdisease, 6)
### Remove missing data from data
heartdisease <- na.omit(heartdisease)
### 75% of the sample size
sample_size <- floor(0.75 * nrow(heartdisease))
## We create the seed to make our partition reproducible
set.seed(1000)
train_index <- sample(seq_len(nrow(heartdisease)), size = sample_size)
## We create separate training and test sets for predictors and labels
heartlabel_train <- heartdisease[train_index,16]
heartlabel_test <- heartdisease[-train_index,16]
heartdisease_train <- heartdisease[train_index,-16]
heartdisease_test <- heartdisease[-train_index,-16]
# We first try with K = 1
my_knn <- knn(heartdisease_train, heartdisease_test, heartlabel_train, k = 1)
table_knn <- table(my_knn, heartlabel_test)
accuracy_heart_test <- sum(diag(table_knn))/ sum(table_knn)
accuracy_heart_test
# We plot the accuracy for different values of K
M = 150 # the number of possible values of K
K <- seq(1, M)
accuracy_heart_test <- rep(0,M)
for (i in 1:M)
{
my_knn <- knn(heartdisease_train, heartdisease_test, heartlabel_train, k = K[i])
table_knn <- table(my_knn, heartlabel_test)
accuracy_heart_test[i] <- sum(diag(table_knn))/ sum(table_knn)
}
max(accuracy_heart_test)
accuracy_heart_test <- as.data.frame(accuracy_heart_test)
ggplot(accuracy_heart_test, aes(x = K, y = accuracy_heart_test)) + geom_line(col = "red")
accuracy_heart_test
setwd("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Model Selection")
library(ggplot2)
library(tidyr)
library(dplyr)
library(FNN) # For K-nearest neighbor regression
library(MLmetrics) #For RMSE calculation
library(readr)
Real_estate <- read_csv("Real_estate.csv")
View(Real_estate)
realestate = read.csv("Real_estate.csv", head = TRUE, check.names=FALSE)
realestate <- data.frame(realestate)
colnames(realestate) <- c("No","transact_date", "house_age", "dist_station", "number_stores", "latitude", "longitude", "price")
head(realestate, 6)
set.seed(1000)
N = 10 # number of training / test sets
M = 150 # the number of possible values of K
K <- seq(3, M+2)
rmse_out_val <- matrix(0, nrow = M, ncol = N)
source("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Model Selection/Cross_Validation.R", echo=TRUE)
#### This file is for an illustration of cross-validation with "Real_estate.csv" data
library(ggplot2)
library(tidyr)
library(dplyr)
library(FNN) # For K-nearest neighbor regression
library(MLmetrics) #For RMSE calculation
################################################################################
############## We first try with the "Real_estate.csv"
realestate = read.csv("Real_estate.csv", head = TRUE, check.names=FALSE)
############## Change the column names for easier implementation
realestate <- data.frame(realestate)
colnames(realestate) <- c("No","transact_date", "house_age", "dist_station", "number_stores", "latitude", "longitude", "price")
head(realestate, 6)
set.seed(1000)
N = 10 # number of training / test sets
M = 150 # the number of possible values of K
K <- seq(3, M+2)
rmse_out_val <- matrix(0, nrow = M, ncol = N)
### 75% of the sample size for the training set
sample_size <- floor(0.75 * nrow(realestate))
for (i in 1:N)
{
train_index <- sample(seq_len(nrow(realestate)), size = sample_size)
realestate_train <- realestate[train_index,]
realestate_test <- realestate[-train_index,]
age_train <- data.frame(realestate_train$house_age)
age_test <- data.frame(realestate_test$house_age)
price_train <- data.frame(realestate_train$price)
price_test <- as.vector(realestate_test$price)
for (j in 1:M)
{
knnfit = knn.reg(age_train, age_test, price_train, k = K[j])
rmse_out_val[j,i] = RMSE(knnfit$pred, price_test)
}
}
### Plot the RMSE for 10 training/ test sets at different K
new_rmse_out_val <- as.data.frame(rmse_out_val)
plot <- ggplot(data = new_rmse_out_val, aes(x = K))
for (i in 1:N)
{
plot <- plot + geom_line( aes_string( y = new_rmse_out_val[,i]), col = i+2)
}
plot + ylab('rmse_out')
average_rmse_out_val <- rowMeans(rmse_out_val) #We take the average of each row in the matrix rmse_out_val
average_rmse_out_val <- as.data.frame(average_rmse_out_val)
ggplot(average_rmse_out_val, aes(x = K, y = average_rmse_out_val)) + geom_line(col = "red")
#### We first need to remove missing data from the dataset
## We first create a data frame with only column real estate price and house age
myvars <- c("house_age", "price")
realestate_sub <- realestate[myvars]
realestate_sub <- realestate_sub[complete.cases(realestate_sub), ] #Remove missing data
n = nrow(realestate_sub) #Number of data in the real estate dataset, amount of training/ test sets we have in LOOCV
error_rate <- rep(0, M) #We create a vector of error rates with size M for the M values of K
for (i in 1:M)
{
for (j in 1:n)
{
age_train <- realestate_sub$house_age[-j]
age_test <- realestate_sub$house_age[j]
price_train <- realestate_sub$price[-j]
price_test <- realestate_sub$price[j]
knnfit = knn.reg(age_train, age_test, price_train, k = K[i])
error_rate[i] <- error_rate[i] + RMSE(knnfit$pred, price_test)
}
error_rate[i] <- error_rate[i]/ n
}
## Plot the RMSE values
error_rate <-as.data.frame(error_rate)
ggplot(error_rate, aes(x = K, y = error_rate)) + geom_line()
### We divide data into K folds
K = 10
N = nrow(realestate_sub)
fold_index = rep_len(1:K, N) #We repeat the sequence (1,2...,K) until reach size N
fold_index = sample(fold_index, replace = FALSE) #We randomly permute the elements of fold_index
M = 10 #The number of possible degress of polynomial regression
fold_error_rate <- matrix(0, nrow = M, ncol = K)
for (i in 1:M)
{
for (j in 1:K)
{
train_index = which(fold_index != j)
realestate_sub_train = realestate_sub[train_index,]
realestate_sub_test = realestate_sub[-train_index,]
polyfit <- lm(price~poly(house_age,i), data = realestate_sub_train)
pricetest_pred <- predict(polyfit, data = realestate_sub_test)
fold_error_rate[i,j] <- RMSE(pricetest_pred, realestate_sub_test$price)
}
}
### Plot the RMSE for K folds
new_fold_error_rate <- as.data.frame(fold_error_rate)
plot <- ggplot(data = new_fold_error_rate, aes(x = seq(1:M)))
for (i in 1:K)
{
plot <- plot + geom_line( aes_string( y = new_fold_error_rate[,i]), col = i+2)
}
plot + xlab("Degree of Polynomial") + ylab("Error rate")
mean_fold_error_rate <- rowMeans(fold_error_rate)
mean_fold_error_rate <- as.data.frame(mean_fold_error_rate)
ggplot(mean_fold_error_rate, aes(x = seq(1:M), y = mean_fold_error_rate)) + geom_line() + xlab("Degree of Polynomial") + ylab("Error rate")
library(car)
cs65692 <- Robey
library(car)
cs65692 <- Robey
library(car)
library(ggplot2)
library(dplyr)
cs65692 <- Robey
source("~/Documents/GitHub/Personal Projects/R/SDS320/UntitledR.R", echo=TRUE)
iqr_tfr <- IQR(Robey$tfr, na.rm=TRUE)
source("~/Documents/GitHub/Personal Projects/R/SDS320/UntitledR.R", echo=TRUE)
ggplot(Robey, aes(x = region, y = tfr, fill = region)) +
geom_boxplot() +
labs(title = "Comparison of Total Fertility Rate Across Regions",
x = "World Region",
y = "Total Fertility Rate (TFR)") +
theme_minimal()
# Compute mean TFR by region
mean_tfr_by_region <- Robey %>%
group_by(region) %>%
summarise(mean_tfr = mean(tfr, na.rm = TRUE))
# Print mean TFR by region
print(mean_tfr_by_region)
ggplot(Robey, aes(x = region, y = tfr, fill = region)) +
geom_boxplot() +
labs(title = "Comparison of Total Fertility Rate Across Regions",
x = "World Region",
y = "Total Fertility Rate (TFR)") +
theme_minimal()
# Compute mean TFR by region
stat_tfr_by_region <- Robey %>%
group_by(region) %>%
summarise(mean_tfr = mean(tfr, na.rm = TRUE),
median_tfr = median(tfr, na.rm=TRUE),
sd_tfr = sd(tfr, na.rm=TRUE),
iqr_tfr = IQR(tfr, na.rm=TRUE),
min_tfr = min(tfr, na.rm=TRUE),
max_tfr = max(tfr, na.rm=TRUE))
# Print mean TFR by region
print(stat_tfr_by_region)
cs65692
source("~/Documents/GitHub/Personal Projects/R/SDS320/UntitledR.R", echo=TRUE)
source("~/Documents/GitHub/Personal Projects/R/SDS320/UntitledR.R", echo=TRUE)
source("~/Documents/GitHub/Personal Projects/R/SDS320/UntitledR.R", echo=TRUE)
correlation <- cor(x1, x2)
print(paste("Correlation between x1 and x2:", round(correlation, 3)))
# Part (b) - Create scatterplot
plot(x1, x2, main = paste("Scatterplot of x1 vs x2 (Correlation = ", round(correlation, 3), ")"),
xlab = "x1", ylab = "x2", pch = 19, col = "blue")
grid()
model <- lm(y ~ x1 + x2)
summary(model)
model_x1 <- lm(y ~ x1)
summary(model_x1)
model_x2 <- lm(y ~ x2)
summary(model_x2)
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
model <- lm(y ~ x1 + x2)
summary(model)
model_x1 <- lm(y ~ x1)
summary(model_x1)
model_x2 <- lm(y ~ x2)
summary(model_x2)
set.seed(2)
x <- rnorm(100)
y <- 2*x+rnorm(100)
model <- lm(y~x)
summary(model)
set.seed(2)
x <- rnorm(100)
y <- 2*x+rnorm(100)
model <- lm(y~x+0)
summary(model)
model <- lm(x~y+0)
summary(model)
model <- lm(y ~ x + 0)
summary(model)  # Get t-statistic from the model
# Extract necessary values
n <- length(x)
sum_x2 <- sum(x^2)
sum_y2 <- sum(y^2)
sum_xy <- sum(x * y)
# Compute the t-statistic using the given formula
t_stat_formula <- (sqrt(n - 1) * sum_xy) / sqrt((sum_x2 * sum_y2) - (sum_xy^2))
# Extract the t-statistic from the model summary
t_stat_model <- summary(model)$coefficients[1, 3]
# Print the results
cat("t-statistic from regression model:", t_stat_model, "\n")
cat("t-statistic from formula:", t_stat_formula, "\n")
source("~/Documents/GitHub/Personal Projects/R/SDS320/UntitledR.R", echo=TRUE)
model <- lm(y ~ x + 0)
summary(model)
n <- length(x)
sum_x2 <- sum(x^2)
sum_y2 <- sum(y^2)
sum_xy <- sum(x * y)
t_stat_formula <- (sqrt(n - 1) * sum_xy) / sqrt((sum_x2 * sum_y2) - (sum_xy^2))
t_stat_model <- summary(model)$coefficients[1, 3]
cat("t-statistic from regression model:", t_stat_model, "\n")
cat("t-statistic from formula:", t_stat_formula, "\n")
model_y_on_x <- lm(y ~ x)
summary(model_y_on_x)
t_stat_y_on_x <- summary(model_y_on_x)$coefficients[2, 3]
model_x_on_y <- lm(x ~ y)
summary(model_x_on_y)
t_stat_x_on_y <- summary(model_x_on_y)$coefficients[2, 3]
cat("t-statistic for regression of y onto x:", t_stat_y_on_x, "\n")
cat("t-statistic for regression of x onto y:", t_stat_x_on_y, "\n")
set.seed(2)
x <- rnorm(100)
y <- 2*x+rnorm(100)
model_y_on_x <- lm(y ~ x)
summary(model_y_on_x)
t_stat_y_on_x <- summary(model_y_on_x)$coefficients[2, 3]
model_x_on_y <- lm(x ~ y)
summary(model_x_on_y)
t_stat_x_on_y <- summary(model_x_on_y)$coefficients[2, 3]
cat("t-statistic for regression of y onto x:", t_stat_y_on_x, "\n")
cat("t-statistic for regression of x onto y:", t_stat_x_on_y, "\n")
setwd("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Model Selection")
install.packages("leaps")
library(leaps)
library(ggplot2)
library(tidyr)
library(dplyr)
library(leaps)
realestate = read.csv("Real_estate.csv", head = TRUE, check.names=FALSE)
realestate <- data.frame(realestate)
colnames(realestate) <- c("No","transact_date", "house_age", "dist_station", "number_stores", "latitude", "longitude", "price")
head(realestate, 6)
realestate <- realestate[,-1]
### We use "regsubsets" operator to perform best subset selection. It chooses the best set of variables for each model size
my_regsubset <- regsubsets(price~., realestate)
summary(my_regsubset)
### Take a look at the values of BIC, Adjusted R^2, AIC, C_p
my_summary <- summary(my_regsubset)
names(my_summary)
# With C_p - smallest
ggplot(as.data.frame(my_summary$cp), aes(x = seq(1:6), y = my_summary$cp)) + geom_line(color = "red") + geom_point(size = 1) + xlab("Number of predictors") + ylab("C_p")
# With BIC - smallest
ggplot(as.data.frame(my_summary$bic), aes(x = seq(1:6), y = my_summary$bic)) + geom_line(color = "blue") + geom_point(size = 1) + xlab("Number of predictors") + ylab("BIC")
e
# With adjusted R^2 - largest
ggplot(as.data.frame(my_summary$adjr2), aes(x = seq(1:6), y = my_summary$adjr2)) + geom_line(color = "pink") + geom_point(size = 1) + xlab("Number of predictors") + ylab("Adjusted R2")
forward_selec <- regsubsets(price~., realestate, method = "forward")
summary(forward_selec)
my_summary <- summary(forward_selec)
# With C_p - smallest
ggplot(as.data.frame(my_summary$cp), aes(x = seq(1:6), y = my_summary$cp)) + geom_line(color = "red") + geom_point(size = 1) + xlab("Number of predictors") + ylab("C_p")
# With BIC - smallest
ggplot(as.data.frame(my_summary$bic), aes(x = seq(1:6), y = my_summary$bic)) + geom_line(color = "blue") + geom_point(size = 1) + xlab("Number of predictors") + ylab("BIC")
# With adjusted R^2 - largest
ggplot(as.data.frame(my_summary$adjr2), aes(x = seq(1:6), y = my_summary$adjr2)) + geom_line(color = "pink") + geom_point(size = 1) + xlab("Number of predictors") + ylab("Adjusted R2")
rd_selec <- regsubsets(price~., realestate, method = "backward")
summary(backward_selec)
backward_selec <- regsubsets(price~., realestate, method = "backward")
summary(backward_selec)
my_summary <- summary(backward_selec)
### Create plots with these values
# With C_p - smallest
ggplot(as.data.frame(my_summary$cp), aes(x = seq(1:6), y = my_summary$cp)) + geom_line(color = "red") + geom_point(size = 1) + xlab("Number of predictors") + ylab("C_p")
# With BIC - smallest
ggplot(as.data.frame(my_summary$bic), aes(x = seq(1:6), y = my_summary$bic)) + geom_line(color = "blue") + geom_point(size = 1) + xlab("Number of predictors") + ylab("BIC")
# With adjusted R^2 - largest
ggplot(as.data.frame(my_summary$adjr2), aes(x = seq(1:6), y = my_summary$adjr2)) + geom_line(color = "pink") + geom_point(size = 1) + xlab("Number of predictors") + ylab("Adjusted R2")
setwd("~/Documents/GitHub/Personal Projects/R/SDS320")
setwd("~/Documents/GitHub/Personal Projects/R/Elements of Statistical Machine Learning/Model Selection")
library(ggplot2)
library(tidyr)
library(dplyr)
library(MLmetrics)
# Install packges for ridge regression and the Lasso
install.packages("glmnet")
library(glmnet)
install.packages("seriation")
library(seriation)
realestate = read.csv("Real_estate.csv", head = TRUE, check.names=FALSE)
realestate <- data.frame(realestate)
colnames(realestate) <- c("No","transact_date", "house_age", "dist_station", "number_stores", "latitude", "longitude", "price")
head(realestate, 6)
### We remove column "No", "latitude", and "longtitude"
realestate <- realestate[,c(-1, -6, -7)]
## We first change data into appropriate forms
response_val <- realestate$price
predictors_val <- model.matrix(price~.,realestate)[,-1]
######### We first try ridge regression when lambda  = 1
my_ridge <- glmnet(predictors_val, response_val, alpha = 0, lambda = 1)
#alpha = 0 means we perform ridge regression
coef(my_ridge)
my_ridge <- glmnet(predictors_val, response_val, alpha = 0, lambda = 100) #alpha = 0 means we perform ridge regression
coef(my_ridge)
lambda_val <- seq(1000,0, length = 100)
my_ridge <- glmnet(predictors_val, response_val, alpha = 0, lambda = lambda_val)
## Output the coefficients of ridge regression
coef(my_ridge)
coef_my_ridge <- as.matrix(coef(my_ridge))
order_seq <- seq(100, 1, length = 100)
coef_my_ridge <- permute(coef_my_ridge, ser_permutation(NA, order_seq))
## Plot the coefficients of ridge regression versus the values of lambda
coef_my_ridge <- t(coef_my_ridge)
coef_my_ridge <- as.data.frame(coef_my_ridge)
plot <- ggplot(data = coef_my_ridge, aes(x = seq(0,1000, length = 100)))
for (i in 1:4)
{
plot <- plot + geom_line( aes_string(y = as.vector(coef_my_ridge[,i+1])), col = i+2)
}
plot + xlab('lambda') + ylab('estimated coefficients')
my_lasso <- glmnet(predictors_val, response_val, alpha = 1, lambda = 1) #alpha = 1 means we perform the lasso
coef(my_lasso)
######### We then try the lasso when lambda  = 2
my_lasso <- glmnet(predictors_val, response_val, alpha = 1, lambda = 6) #alpha = 1 means we perform the lasso
coef(my_lasso)
lambda_val <- seq(10,0, length = 100)
my_lasso <- glmnet(predictors_val, response_val, alpha = 1, lambda = lambda_val) #alpha = 1 means we perform the lasso
## Output the coefficients of the lasso
coef(my_lasso)
## Permute the columns of matrix
coef_my_lasso <- as.matrix(coef(my_lasso))
order_seq <- seq(100, 1, length = 100)
coef_my_lasso <- permute(coef_my_lasso, ser_permutation(NA, order_seq))
## Plot the coefficients of ridge regression versus the values of lambda
coef_my_lasso <- t(coef_my_lasso)
coef_my_lasso <- as.data.frame(coef_my_lasso)
plot <- ggplot(data = coef_my_lasso, aes(x = seq(0,10, length = 100)))
for (i in 1:4)
{
plot <- plot + geom_line( aes_string(y = as.vector(coef_my_lasso[,i+1])), col = i+2)
}
plot + xlab('lambda') + ylab('estimated coefficients')
set.seed(1000)
N = 10 # number of training / test sets
M = 100 # number of values of lambda
lambda_val <- seq(0,10, length = M) # sequence of values of lambda
rmse_out_val <- matrix(0, nrow = M, ncol = N)
### 75% of the sample size for the training set
### Sample size = 310
sample_size <- floor(0.75 * nrow(realestate))
#$sample_size <- 3
for (i in 1:N)
{
train_index <- sample(seq_len(nrow(realestate)), size = sample_size)
realestate_train <- realestate[train_index,]
realestate_test <- realestate[-train_index,]
response_train_val <- realestate_train$price
predictors_train_val <- model.matrix(price~.,realestate_train)[,-1]
response_test_val <- realestate_test$price
predictors_test_val <- model.matrix(price~.,realestate_test)[,-1]
for (j in 1:M)
{
my_lasso <- glmnet(predictors_train_val, response_train_val, alpha = 1, lambda = lambda_val[j])
my_lasso_pred <- predict(my_lasso, s = lambda_val[j], predictors_test_val)
rmse_out_val[j,i] = RMSE(my_lasso_pred, response_test_val)
}
}
### Plot the RMSE for 10 training/ test sets at different lambda
new_rmse_out_val <- as.data.frame(rmse_out_val)
plot <- ggplot(data = new_rmse_out_val, aes(x = lambda_val))
for (i in 1:N)
{
plot <- plot + geom_line( aes_string( y = new_rmse_out_val[,i]), col = i+2)
}
plot + ylab('rmse_out')
#### Take the average of error rates over N training/ test sets
average_rmse_out_val <- rowMeans(rmse_out_val) #We take the average of each row in the matrix rmse_out_val
average_rmse_out_val <- as.data.frame(average_rmse_out_val)
ggplot(average_rmse_out_val, aes(x = lambda_val, y = average_rmse_out_val)) + geom_line(col = "red")
