---
title: "Clustering"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r global_options, include=FALSE}
# The following code is a basic setup of options for your document
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE, 
                      warning = TRUE,
                      message = FALSE,
                      fig.align = "center", 
                      R.options = list(max.print=50))

# Edit the file starting below

```

In this worksheet, we will introduce an algorithm to look for some potential grouping in our data.

## 1. Dataset and Libraries

We will use a new package today containing the functions related to clustering:

```{r, eval=FALSE}
# Install new packages (only needed once!)
install.packages("cluster")
```

We will also use the `tidyverse`, `ade4` to access a built-in dataset, and `factoextra` to provide information about the algorithm for clustering:

```{r, message=FALSE}
# Load packages
library(tidyverse)
library(ade4)
library(factoextra)
library(cluster)
```

Remember the `atheletes` dataset? It contains information about the performance of 33 athletes in the 10 disciplines of the decathlon at the Olympics in 1988:

```{r}
# Save the database into your environment, then the dataset
data("olympic")
athletes <- olympic$tab

# Quick cleanup
athletes <- athletes |>
  # Translate the variable names (from French!) and reorder
  select(time_100 = `100`, time_110 = `110`,
         time_400 = `400`, time_1500 = `1500`,
         disc = disq, weight = poid, high_jump = haut,
         long_jump = long, javelin = jave, perch = perc)
head(athletes)
```

We would like to see if these athletes cluster together in some groups.

## 2. Clustering

The goal of clustering is to identify observations that are alike/close to each other. We will consider the algorithm for k-means clustering:

1.  Pick *k* points of the *n* observations at random to serve as initial cluster centers.

2.  Assign each *n-k* observation to the cluster whose center is closest.

3.  For each group, calculate means and use them as new centers.

4.  Repeat steps 2-3 until groups stabilize.

Before we apply the algorithm, we need to prepare the data so that all variables are on the same scale.

### a. Prepare the data

We should scale our variables before clustering so that variables can be comparable regardless of their scale.

```{r}
# Prepare the dataset
athletes_scaled <- athletes |> 
  # Scale the variables
  scale() |>
  # Save as a data frame
  as.data.frame()

# Take a look at the scaled data
head(athletes_scaled)
```

All variables are on the same "unitless" scale!

### b. Apply the algorithm

Let's first focus on 2 variables in the `athletes` dataset, `dist_100` and `disc`. We use the `kmeans(data, k = nb_clusters)` function with `k = 2` to find 2 clusters:

```{r}
# For reproducible results: why?
set.seed(322)

# Use the function kmeans() to find clusters
kmeans_results <- athletes_scaled |>
  select(time_100, disc) |>
  kmeans(centers = 2) # centers sets the number of clusters to find

# The output provides some information about the clusters and creates 9 different objects
names(kmeans_results)
```

We will focus on the `cluster` object:

```{r}
# A vector attributing a cluster number to each observation
kmeans_results$cluster
```

The `cluster` object indicates which observations (i.e., which athlete) is in which cluster.

### c. Visualize and interpret the clusters

We can save the identification of the cluster for each observation in the original dataset to manipulate the observations for each cluster:

```{r}
# Consider the original dataset
athletes |>
  # Save cluster assignment as a new variable
  mutate(cluster = as.factor(kmeans_results$cluster)) |>
  # Only keep the variables of interest
  select(time_100, disc, cluster) |>
  head()
```

What characteristics do the athletes share in each cluster? We can visualize the clusters and create summary statistics of each variable to understand some characteristics about the clusters.

#### **Try it! Using the original dataset, visualize the relationship between `time_100` and `disc` for each cluster. Also find the mean and standard deviation for the variables of `time_100` and `disc` for each cluster. Are there any any differences between the clusters?**

```{r}
# Write and submit code here!

```

**Write sentences here!**


We compared 2 clusters but how did we decide that our athletes should be separated into 2 groups?

### d. Choose the number of clusters

Determining the number of clusters to use can be tricky. We can either consider the context or using measures such as the average silhouette width (which measures how cohesive and separated clusters are, simultaneously) for multiple values of `k`. A high average silhouette width indicates a good clustering structure: the observations within the groups are close to each other and the groups are very distinct from each other. We can use the function `fviz_nbclust(scaled_data, clustering_algorithm, method)` to compare different values of `k`:

```{r}
# Maximize the silhouette while keeping a small number of clusters
fviz_nbclust(athletes_scaled, kmeans, method = "silhouette")
```

The average silhouette width seems to indicate that 3 clusters maximize the average width silhouette for the `athletes_scaled`.

#### **Try it! Split the athletes in 3 clusters with `kmeans`. How do the athletes compare now between the 3 clusters?**

```{r}
# Write and submit code here!

```

**Write sentences here!**


### e. Include more variables

What if we would like to consider more variables to compare the athletes? We can technically use all the variables in the `athletes` dataset!

From above, the average silhouette width indicates that we should consider 3 clusters:

```{r}
# For reproducible results
set.seed(322)

# Use the function kmeans() to find clusters
kmeans_results <- athletes_scaled |>
  kmeans(centers = 3)
```

Visualize the clusters with `fviz_cluster()`. 

```{r}
# Let's visualize our data with cluster assignment
fviz_cluster(kmeans_results, data = athletes_scaled)
```

What do the labels of the x-axis and y-axis indicate? Why?

**Write sentences here!**


What characteristics do the athletes share within each cluster? Let's describe each cluster with the mean:

```{r}
# Create basic summary statistics for each cluster in original units
athletes |>
  # Save cluster assignment as a new variable
  mutate(cluster = as.factor(kmeans_results$cluster)) |>
  # For each cluster
  group_by(cluster) |>
  # Find the mean of all variables
  summarize_all(mean)
```

Looking at these means, is there a cluster of athletes that performs consistently better for timed disciplines? worse?

**Write sentences here!**

What about for other events?

**Write sentences here!**


*Note: we can only find the Euclidean distance between numeric variables but there are other distances that could also include categorical variables if we had any.*

------------------------------------------------------------------------

## **Your turn!**

Let's practice some clustering on the `pokemon` dataset.

```{r}
# Upload data from GitHub
pokemon <- read_csv("https://raw.githubusercontent.com/laylaguyot/datasets/main//pokemon.csv")

# Take a look 
head(pokemon)
```

1.  Only consider the variables `HP`, `Attack`, `SpAtk`, `Defense`, `SpDef`, and `Speed`. Prepare the data for clustering by scaling these variables. Name the resulting dataset as `pokemon_scaled`.

```{r}
# Write and submit code here!

```

2.  How many clusters should we find?

```{r}
# Write and submit code here!

```

**Write sentences here!**

3.  Apply the algorithm with the corresponding number of clusters. Visualize the clusters with `fviz_cluster`.

```{r}
# Write and submit code here!

```

4. Add a variable to the original `pokemon` dataset that describes which cluster each Pokemon belongs to. Create summary statistics to describe the Pokemon in each cluster. Do you notice anything?

```{r}
# Write and submit code here!

```

**Write sentences here!**
