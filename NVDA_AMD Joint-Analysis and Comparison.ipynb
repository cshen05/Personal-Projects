{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#financial strength & growth\n",
    "#uses 10-K and 10-Q filings from SEC's EDGAR database \n",
    "#this script parses XML data from company filings and converts them into workable dataframes\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing company, file version, and date\n",
    "file = '10-Q'\n",
    "end = '20230129' #2023-01-29\n",
    "cik1 = '0001045810' #NVDA\n",
    "cik2 = '0000002488' #AMD\n",
    "\n",
    "xmlList1 = []\n",
    "xmlList2 = []\n",
    "\n",
    "xmlList1.append('https://www.sec.gov/ix?doc=/Archives/edgar/data/0001045810/000104581023000017/nvda-20230129_htm.xml')\n",
    "xmlList2.append('https://www.sec.gov/ix?doc=/Archives/edgar/data/0000002488/000000248823000047/amd-20221231_htm.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type={}&dateb={}\"\n",
    "urlstr1 = requests.get(url.format(cik1, file, end)).text\n",
    "urlstr2 = requests.get(url.format(cik2, file, end)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(urlstr1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtableFile2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[1;32m      6\u001b[0m     cells \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#loop through files separated by year\n",
    "soup = BeautifulSoup(urlstr1, 'html.parser')\n",
    "table = soup.find('table', class_='tableFile2')\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 3:\n",
    "        #Parsing all suitable XML files\n",
    "        if '2019' or '2020' or '2021' or '2022' or '2023' in cells[3].text:\n",
    "            html = 'https://www.sec.gov' + cells[1].a['href']\n",
    "            htmlstr = requests.get(html).text\n",
    "            soup = BeautifulSoup(htmlstr, 'html.parser')\n",
    "            table = soup.find('table', class_='tableFile', summary='Data Files')\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) > 3:\n",
    "                    xmlink = 'https://www.sec.gov' + cells[2].a['href']\n",
    "                    xmlList1.append(xmlink)\n",
    "                    break   \n",
    "\n",
    "soup = BeautifulSoup(urlstr2, 'html.parser')\n",
    "table = soup.find('table', class_='tableFile2')\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 3:\n",
    "        #Parsing all suitable XML files\n",
    "        if '2019' or '2020' or '2021' or '2022' or '2023' in cells[3].text:\n",
    "            html = 'https://www.sec.gov' + cells[1].a['href']\n",
    "            htmlstr = requests.get(html).text\n",
    "            soup = BeautifulSoup(htmlstr, 'html.parser')\n",
    "            table = soup.find('table', class_='tableFile', summary='Data Files')\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) > 3:\n",
    "                    xmlink = 'https://www.sec.gov' + cells[2].a['href']\n",
    "                    xmlList2.append(xmlink)\n",
    "                    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
